{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的两个数组相加案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1,2], name='a')\n",
    "b = tf.constant([3,4], name='b')\n",
    "result = a + b\n",
    "sess = tf.Session()\n",
    "sess.run(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 计算图Graph\n",
    "---\n",
    "```python\n",
    "import tensorflow as tf\n",
    "tf.Graph()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 计算图的使用\n",
    "---\n",
    "每个计算图相互独立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph1 v =  [0.]\n",
      "Graph2 v =  [1.]\n"
     ]
    }
   ],
   "source": [
    "# 创建两个计算图，并定义一个初始化变量v\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    v = tf.get_variable('v', shape=[1], initializer=tf.zeros_initializer)\n",
    "    \n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    v = tf.get_variable('v', shape=[1], initializer=tf.ones_initializer)\n",
    "    \n",
    "# 获取g1中的v\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        print('Graph1 v = ', sess.run(tf.get_variable('v')))\n",
    "\n",
    "# 获取g2中的v\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        print('Graph2 v = ', sess.run(tf.get_variable('v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定图在GPU计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "a = tf.constant([1,2], name='a')\n",
    "b = tf.constant([3,4], name='b')\n",
    "\n",
    "with g.device('/gpu:0'):\n",
    "    result = a + b\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tensor\n",
    "---\n",
    "### 1.2.1 introduction\n",
    "---\n",
    "1. tensor\n",
    "    - 0阶张量为标量(scalar)，对应一个数\n",
    "    - 一阶张量为向量(vector)，对应一维数组\n",
    "    - n阶张量对应n维数组\n",
    "2. tensor的数据结构\n",
    "    - name: `name:src_output`\n",
    "    - shape:\n",
    "    - dtype:\n",
    "3. tensor支持的数据类型\n",
    "    - tf.float32, tf.float64\n",
    "    - tf.int8, tf.int16, tf.int32, tf.int64, tf.uint8\n",
    "    - tf.bool\n",
    "    - tf.complex64, tf.complex128\n",
    "4. 张量的使用\n",
    "    - 对中间计算结果的引用\n",
    "    - 可以通过name获取保存的计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([1,2], name='a')\n",
    "b = tf.constant([3,4], name='b')\n",
    "result = tf.add(a, b, name='add_ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_ab:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "# '节点名:该节点的第几个输出'，如果重复赋值，以前的不会被覆盖，会出现`add_ab_1:0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'a_12:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Session\n",
    "---\n",
    "tensorflow 通过使用Session来执行定义好的运算。\n",
    "```python\n",
    "sess = tf.Session()\n",
    "# 执行定义好的张量\n",
    "sess.run()\n",
    "\n",
    "sess.close()\n",
    "```\n",
    "or\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    sess.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 定义默认会话\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方法1**：使用as_default创建一个context manager，在内部可以直接使用tensor_name.eval()执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6]\n"
     ]
    }
   ],
   "source": [
    "# 使用as_default创建一个context manager，在内部可以直接使用tensor_name.eval()执行。\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(result.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方法2**：使用`tf.InteractiveSession()`函数,直接创建一个默认session，不用再去指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(result.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方法3**：使用`ConfigProto()`创建一个config对象，在session创建的时候指定config属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                       log_device_placement=True)\n",
    "\n",
    "sess1 = tf.InteractiveSession(config=config)\n",
    "# sess2 = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 实现神经网络\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0.7, 0.9], shape=(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.constant([[0.2, 0.3],[0.1,-0.5],[0.4,0.2]], shape=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = tf.constant([0.6, 0.1, -0.2], shape=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.matmul(w1, x)\n",
    "y = tf.matmul(w2, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11600002]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 variable\n",
    "---\n",
    "在tensorflow中，一个变量值在被使用之前，这个变量的**初始化过程**要被明确调用。变量定义类似于python中的函数定义，想要使用，需要调用。通过session调用。\n",
    "```python\n",
    "# 创建variable,并将变量加入到GraphKeys.VARIABLES,如果参数trainable=True,默认为True,则变量会被加入到GraphKeys.TRAINABLE_VARIABLES\n",
    "v = tf.Variable()\n",
    "# 初始化\n",
    "v.initializer\n",
    "\n",
    "# 获取已定义的变量列表\n",
    "tf.global_variables()\n",
    "\n",
    "# 获取可训练的变量列表\n",
    "tf.trainable_variables\n",
    "\n",
    "```\n",
    "\n",
    ">  随机生成函数\n",
    "---\n",
    "```python\n",
    "tf.random_normal\n",
    "tf.truncated_normal\n",
    "tf.random_uniform\n",
    "tf.random.gamma\n",
    "```\n",
    "\n",
    "> 常数生成函数\n",
    "---\n",
    "```python\n",
    "tf.zeros\n",
    "tf.ones\n",
    "tf.fill\n",
    "tf.constant\n",
    "```\n",
    "### 1.4.2 前向传播\n",
    "---\n",
    "1. 定义输入值\n",
    "2. 定义隐藏层1的权重\n",
    "3. 定义隐藏层2的权重\n",
    "4. 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7798672]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义输入值\n",
    "x = tf.constant([0.7,0.9], shape=(2, 1), dtype=tf.float32)\n",
    "\n",
    "# 随机生成权重值，采用正太分布\n",
    "w1 = tf.Variable(tf.random_normal((3, 2), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((1, 3), stddev=1, seed=1))\n",
    "\n",
    "# 定义计算过程\n",
    "h = tf.matmul(w1, x)\n",
    "y = tf.matmul(w2, h)\n",
    "\n",
    "# 定义全局变量初始化\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 开始计算\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(w1.initializer)\n",
    "#     sess.run(w2.initializer)\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 placeholder\n",
    "---\n",
    "用placeholder管理常量\n",
    "```python\n",
    "# 创建\n",
    "v = tf.placeholder(dtype, shape=None, name=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7798672]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义输入值\n",
    "x = tf.placeholder(tf.float32, shape=(2, 1), name='input')\n",
    "\n",
    "# 随机生成权重值，采用正太分布\n",
    "w1 = tf.Variable(tf.random_normal((3, 2), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((1, 3), stddev=1, seed=1))\n",
    "\n",
    "# 定义计算过程\n",
    "h = tf.matmul(w1, x)\n",
    "y = tf.matmul(w2, h)\n",
    "\n",
    "# 定义全局变量初始化\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 开始计算\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(y, feed_dict={x: [[0.7],[0.9]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义多维输入值,这里定义三个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7798674 -1.8407464 -3.4529805]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 定义输入值（三个样本）\n",
    "x = tf.placeholder(tf.float32, shape=(2, 3), name='input')\n",
    "\n",
    "# 随机生成权重值，采用正太分布\n",
    "w1 = tf.Variable(tf.random_normal((3, 2), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((1, 3), stddev=1, seed=1))\n",
    "\n",
    "# 定义计算过程\n",
    "h = tf.matmul(w1, x)\n",
    "y = tf.matmul(w2, h)\n",
    "\n",
    "# 定义全局变量初始化\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 定义输入值\n",
    "input_data = np.array([[0.7, 0.9],\n",
    "                      [0.1, 0.4],\n",
    "                      [0.5, 0.8]]).T\n",
    "# 开始计算\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(y, feed_dict={x: input_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 反向传播\n",
    "---\n",
    "这里采用交叉熵\n",
    "$$H(p,q)=-\\sum_{i=1}^np(x_i)log(q(x_i))$$\n",
    "其中p为target值的概率，q为预测值的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.5 完整的代码\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1-->\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2-->\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 , cross entropy is 1.89805\n",
      "After 1000 , cross entropy is 0.655075\n",
      "After 2000 , cross entropy is 0.626172\n",
      "After 3000 , cross entropy is 0.615096\n",
      "After 4000 , cross entropy is 0.610309\n",
      "new w1\n",
      " [[ 0.02476974  0.56948686  1.6921943 ]\n",
      " [-2.1977353  -0.23668927  1.1143897 ]]\n",
      "new w2\n",
      " [[-0.45544702]\n",
      " [ 0.49110925]\n",
      " [-0.98110336]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 定义batch的大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义输入值,这里不具体指定样本的数量\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='input')\n",
    "# 定义输出的label\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='target')\n",
    "\n",
    "# 随机生成权重值，采用正太分布\n",
    "w1 = tf.Variable(tf.random_normal((2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 定义计算过程\n",
    "h = tf.matmul(x, w1)\n",
    "y = tf.matmul(h, w2)\n",
    "\n",
    "# 对输出结果归一化处理\n",
    "y = tf.sigmoid(y)\n",
    "\n",
    "\n",
    "# 求交叉熵，衡量误差\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)) + (1 - y_) * tf.log(tf.clip_by_value(1 - y, 1e-10, 1.0))\n",
    ")\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "# 随机生成模拟数据集\n",
    "rdm = np.random.RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "\n",
    "# 确定target\n",
    "Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]\n",
    "\n",
    "with tf.Session() as sess:   \n",
    "    # 定义全局变量初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    print('w1-->\\n', sess.run(w1))\n",
    "    print('w2-->\\n', sess.run(w2))\n",
    "\n",
    "    steps = 5000\n",
    "    for i in range(steps):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y[start: end]})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x: X, y_: Y})\n",
    "            print('After %d , cross entropy is %g' % (i, total_cross_entropy))\n",
    "    print('new w1\\n', sess.run(w1))\n",
    "    print('new w2\\n', sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 深层NN\n",
    "---\n",
    "## 2.1 常用的非线性激活函数\n",
    "---\n",
    "```python\n",
    "tf.nn.relu()\n",
    "tf.nn.sigmoid()\n",
    "tf.nn.tanh()\n",
    "```\n",
    "\n",
    "## 2.2 损失函数\n",
    "---\n",
    "1. 分类问题\n",
    "    - 交叉熵\n",
    "        - 先用softmax将输出结果转成概率分布的形式`tf.nn.softmax()`\n",
    "        - $$H(p, q) = - \\sum_xlog q(x)$$\n",
    "        - p为正确答案，q为预测答案，这里都可以用概率表示。整体的意义为用q的概率分布来表示p的概率分布的困难程度。\n",
    "        - p和q越接近，交叉熵的值越小。\n",
    "        ```python\n",
    "        # softmax + cross_entropy\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        # 只有一个正确答案的分类问题\n",
    "        tf.nn.softmax_sparse_softmax_cross_entropy_with_logits()\n",
    "        ```\n",
    "2. 回归问题\n",
    "    - MSE(mean squared error)\n",
    "    - $$MSE(y, y^{'}) = \\frac{\\sum_{i=1}^{n}(y_i - y_i^{'})^2}{n}$$\n",
    "    - MSE也常用于分类问题的损失函数\n",
    "    - `tf.reduce_mean(tf.square(y_ - y))`\n",
    "3. 自定义\n",
    "\n",
    "## 2.3 网络优化\n",
    "---\n",
    "### 1. 学习率\n",
    "- 指数衰减法\n",
    "```python\n",
    "    decayed_learning_rate = learning_rate *\n",
    "                    decay_rate ^ (global_step / decay_steps)\n",
    "```\n",
    "- learning_rate:学习率，dency_rate:衰减率，global_step:迭代次数，常为一个数值为0的tensor。decay_steps:衰减速度。eg:每迭代十万次，衰减率乘以0.96\n",
    "```python\n",
    "...\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           100000, 0.96, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "learning_step = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    .minimize(...my loss..., global_step=global_step)\n",
    ")\n",
    "```\n",
    "- 对应tf中的函数为：`tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)`\n",
    "    - staircase为False，表示连续衰减\n",
    "    - staircase为True，表示阶梯衰减。\n",
    "    \n",
    "### 2. 过拟合\n",
    "- 正则化。正则化是处理过拟合常用的方法。**正则化是针对权重进行操作的**\n",
    "- L1正则化$$R(w) = \\lVert{w}\\rVert_1 = \\sum_i\\lvert{w_i}\\rvert$$\n",
    "- L2正则化$$R(w) = \\lVert{w}\\rVert_1^2 = \\sum_i\\lvert{w_i^2}\\rvert$$\n",
    "- L1和L2同时使用：$$R(w) = \\sum_i\\alpha\\lvert{w_i}\\rvert + (1 - \\alpha)w_i^2$$\n",
    "- 如果损失函数为$J(\\theta)$加入了正则化后，加会变成$J(\\theta) + \\lambda{R(w)}$\n",
    "- eg\n",
    "```python\n",
    "weights = tf.constant([[1.0, -2.0],[-3.0, 4.0]])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.contrib.layers.l1_regularizer(0.5)(weights)))\n",
    "    print(sess.run(tf.contrib.layers.l2_regularizer(0.5)(weights)))\n",
    "    print(sess.run(tf.contrib.layers.l1_l2_regularizer(0.5)(weights)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个五层神经网络(3个隐藏层)，带有L2正则化的实现方法。(只是部分实现代码，不做具体实现)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 这里传入一个shape，随机生成一组权重值，并以lambda和权重值得到L2，并加入到tf的collection中\n",
    "def get_weight(shape, lamb):\n",
    "    # 依据给定的shape生成一组正太分布随机值\n",
    "    weights = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    # 计算L2\n",
    "    L2 = tf.contrib.layers.l2_regularizer(lamb)(weights)\n",
    "    tf.add_to_collection('losses', L2)\n",
    "    return weights\n",
    "\n",
    "# 定义输入值\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "# 定义target值结构\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "batch_size = 8\n",
    "# 定义每一层的节点数\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "# 层数\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "# 定义所在计算层\n",
    "cur_layer = x\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1, n_layers):\n",
    "    # 获取第一层的weights，先确定shape\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weight = get_weight((in_dimension, out_dimension), 0.01)\n",
    "    # 更新当前层的输出值\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=(out_dimension, 1)))\n",
    "    cur_layer = tf.matmul(cur_layer, weight) + bias\n",
    "    in_dimension = out_dimension\n",
    "    \n",
    "# 求损失函数，MSE\n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "tf.add_to_collection('losses', mse_loss)\n",
    "\n",
    "# \n",
    "loss = tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "## ing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 滑动平均模型（作用？？）\n",
    "- 在每层的计算中，对权重和偏移量计算一个滑动平均(也理解为对所有神经网络参数的变量上使用滑动平均)\n",
    "- (可以使模型在测试数据上更加的robust).\n",
    "- 适用于**随机梯度下降**训练神经网络时\n",
    "- `tf.train.ExponentialMovingAverage(decay, num_updates=None, zero_debias=False, name='ExponentialMovingAverage')`\n",
    "    - decay越大模型越趋于稳定。\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1=0,step=0\n",
      "[0.0, 0.0]\n",
      "\n",
      "v1=5,step=0\n",
      "[5.0, 4.5]\n",
      "\n",
      "v1=10,step=1000\n",
      "[10.0, 4.555]\n",
      "\n",
      "v1=10, step=1000\n",
      "[10.0, 4.60945]\n"
     ]
    }
   ],
   "source": [
    "# 实现移动平均模型\n",
    "import tensorflow as tf\n",
    "\n",
    "v1 = tf.Variable(0, dtype=tf.float32)\n",
    "step = tf.Variable(0, trainable=False)\n",
    "\n",
    "ema = tf.train.ExponentialMovingAverage(0.99, step)\n",
    "maintain_average_op = ema.apply([v1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print('v1=0,step=0')\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    sess.run(tf.assign(v1, 5))\n",
    "    sess.run(maintain_average_op)\n",
    "    print('\\nv1=5,step=0')\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    sess.run(tf.assign(step, 1000))\n",
    "    sess.run(tf.assign(v1, 10))\n",
    "    sess.run(maintain_average_op)\n",
    "    print('\\nv1=10,step=1000')\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    print('\\nv1=10, step=1000')\n",
    "    sess.run(maintain_average_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))     \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 MNIST\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/softfiles/workspace/tensorflow/data/train-images-idx3-ubyte.gz\n",
      "Extracting D:/softfiles/workspace/tensorflow/data/train-labels-idx1-ubyte.gz\n",
      "Extracting D:/softfiles/workspace/tensorflow/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting D:/softfiles/workspace/tensorflow/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('D:/softfiles/workspace/tensorflow/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,4,3,2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
