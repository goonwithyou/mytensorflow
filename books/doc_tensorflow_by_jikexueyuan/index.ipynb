{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 教程\n",
    "---\n",
    "1. MNIST初级教程\n",
    "2. MNIST高级教程\n",
    "3. TensorFlow使用指南\n",
    "4. 卷积神经网络\n",
    "5. 单词的向量表示\n",
    "6. 循环神经网络\n",
    "7. 序列到序列模型\n",
    "8. Mandelbrot集合\n",
    "9. 偏微分方程\n",
    "10. MNIST数据下载\n",
    "11. 视觉物体识别\n",
    "12. Deep Dream\n",
    "\n",
    "# 3 运作方式\n",
    "1. 综述\n",
    "2. 变量\n",
    "3. TensorBoard：可视化学习\n",
    "4. TensorBoard：图标可视化\n",
    "5. 数据读取\n",
    "6. 线程和队列\n",
    "7. 增加一个Op\n",
    "8. 自定义数据读取\n",
    "9. 使用GPUs\n",
    "10. 共享变量\n",
    "\n",
    "# 4 资源\n",
    "1. 其他资源\n",
    "2. BibTex引用\n",
    "3. 应用实例\n",
    "4. 常见问题\n",
    "5. 术语表\n",
    "6. 张量的阶，形状，数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mnist机器学习入门\n",
    "\n",
    "> ### 1 加载Mnist数据集\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('D:/softfiles/workspace/tensorflow/data/', one_hot=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2 softmax回归\n",
    "\n",
    "**softmax模型**可以用来**给不同的对象分配概率**。在进行模型训练的最后一步都常用softmax来分配概率。\n",
    "\n",
    "softmax的定义为：\n",
    "$$softmax(x) = normalize(exp(x))$$\n",
    "展开等式右边的式子：\n",
    "$$softmax(x)_i = \\frac{exp(x_i)}{\\sum_jexp(x_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3 实现回归模型\n",
    "\n",
    "```python\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "```\n",
    "- x不是一个特定的值，而是一个**占位符**placeholder,只需要在运行的时候赋值即可。\n",
    "\n",
    "```python\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros[10])\n",
    "```\n",
    "一个**Variable**代表一个**可修改的tensor**,存在于tensorflow的用于描述交互性操作的图中。**一般模型参数**都可以用**Variable**表示。\n",
    "\n",
    "```python\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4 训练模型\n",
    "\n",
    "1. 定义损失函数\n",
    "\n",
    "定义损失函数评估模型的好坏。常见的成本函数有**交叉熵**。\n",
    "$$H_{y'}=-\\sum_iy'_ilog(y_i)$$\n",
    "- y是预测的概率分布\n",
    "- $y'$是实际的分布\n",
    "\n",
    "**计算交叉熵**\n",
    "```python\n",
    "y_ = tf.placeholder('float', [None, 10])\n",
    "```\n",
    "所以：\n",
    "```python\n",
    "cross_entropy = - tf.reduce_sum(y_ * tf.log(y))\n",
    "```\n",
    "\n",
    "2. 反向传播，更新权重值\n",
    "\n",
    "```python\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "```\n",
    "这里使用梯度下降算法。\n",
    "\n",
    "Tensorflow他会在后台给描述你的计算的那张图里面增加一系列的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。\n",
    "\n",
    "3. 初始化\n",
    "\n",
    "```python\n",
    "init = tf.initiallize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "# 开始训练\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "```\n",
    "随机抓取训练数据中的100个批次的处理数据点，进行训练。\n",
    "\n",
    "4. 模型评估\n",
    "\n",
    "```python\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.raduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "# 测试训练数据，打印正确率\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Mnist优化\n",
    "---\n",
    "> ### 1 权重初始化\n",
    "\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_mormal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "```\n",
    "\n",
    "> ### 2 定义卷积和池化\n",
    "\n",
    "```python\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        \n",
    "```\n",
    "\n",
    "> ### 3 第一层卷积\n",
    "\n",
    "```python\n",
    "# [5,5,1,32]前两个维度是patch的大小，接着是输入通道的数目，最后是输出的通道数目。\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# [-1, 28, 28, 1]第二三维对应图片的宽和高，最后一维对应图片的颜色通道数。\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# 开始卷积和池化操作\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "```\n",
    "*输出为14 x 14 x 32*\n",
    "\n",
    "> ### 4 第二层卷积\n",
    "\n",
    "```python\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "```\n",
    "*输出为7 x 7 x 64*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 5 密集连接层\n",
    "\n",
    "```python\n",
    "# 加入一个有1024个神经元的全连接层\n",
    "W_fcl = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fcl = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 6 Dropout\n",
    "\n",
    "为了减少过拟合，在输出层之前加入dropout。\n",
    "```python\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fcl_drop = tf.nn.dropout(h_fcl, keep_prob)\n",
    "```\n",
    "\n",
    "> ### 7 输出层\n",
    "\n",
    "```python\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fcl_drop, W_fc2) + b_fc2)\n",
    "```\n",
    "\n",
    "> ### 8 训练和评估模型\n",
    "\n",
    "```python\n",
    "# 交叉熵\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "\n",
    "# 梯度下降\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 判断预测准确性\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "# 准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "# 初始化\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# 开始训练\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # 每100次输出一次准确率\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_:batch[1], keep_prob: 1.0})\n",
    "        print('After %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob: 0.5})\n",
    "\n",
    "# 输出test准确率\n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0\n",
    "}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TensorFlow使用指南\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1 数据准备\n",
    "\n",
    "```python\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "\n",
    "label_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2 构建图表（重点）\n",
    "\n",
    "```python\n",
    "def inference():\n",
    "    # 构建图表, 返回预测结果\n",
    "    \n",
    "    # 每一层的操作都创建一个域\n",
    "#     with tf.name_scope('hidden1') as scope;\n",
    "#         weights = tf.Variable(tf.truncated_normal([IMAGE_PIXES],stddev=1.0/math.sqrt(float(IMAGE_PIXES))), name='weight')\n",
    "#         biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "     \n",
    "    # 图表的主要操作\n",
    "    # hidden1 = tf.nn.relu(tf.matmul(images, weights1) + biases1)\n",
    "    # hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    # logits = tf.matmul(hidden2, weights) + biases\n",
    "    pass\n",
    "\n",
    "def loss():\n",
    "    # 往inference图表中添加loss\n",
    "    \n",
    "    # 处理label数据\n",
    "#     batch_size = tf.size(labels)\n",
    "#     labels = tf.expand_dims(labels, 1)\n",
    "#     indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "#     concated = tf.concat(1, [indices, labels])\n",
    "#     onehot_labels = tf.sparse_to_dense(concated, tf,pack([batch_size, NUM_CLASSES]), 1.0. 0.0)\n",
    "    \n",
    "#     cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, onehot_labels, name='xentropy')\n",
    "    \n",
    "#     loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def training():\n",
    "    # 添加梯度操作,将损失最小化。\n",
    "    # 从loss函数中获取损失Tensor，将其交给tf.scalar_summary,在与SummaryWriter配合使用，可以向事件文件event file\n",
    "    # 中生成汇总值summary values。每次写入汇总值时，他都会释放损失Tensor的当前值snapshot value.\n",
    "    \n",
    "#     tf.scalar_summary(loss.op.name, loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(FLAGS, learning_rate)\n",
    "    \n",
    "    # 之后生成一个变量用于保存全局训练步骤global training step的数值，并使用`minimize()`\n",
    "    # 函数更新系统中的三角权重，增加全局步骤的操作。\n",
    "    # 这个操作称为`train_op`，是TensorFlow会话session诱发一个完整训练步骤所必须运行的操作。\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3 训练模型\n",
    "\n",
    "1. **图表**\n",
    "\n",
    "```python\n",
    "# 将所有已经构建的操作都与默认的Graph全局实例关联起来。\n",
    "with tf.Graph().as_default():\n",
    "    # 初始化所有variable\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "```\n",
    "\n",
    "2. **会话**\n",
    "\n",
    "\n",
    "```python\n",
    "# 构建session用于运行图表\n",
    "sess = tf.Session()\n",
    "# or\n",
    "with tf.Session() as sess:\n",
    "```\n",
    "\n",
    "3. **训练循环**\n",
    "\n",
    "```python\n",
    "# 开始训练\n",
    "for step in range(max_step):\n",
    "    sess.run(train_op)\n",
    "```\n",
    "\n",
    "4. **向图表提供反馈**\n",
    "\n",
    "`fill_feed_dict`函数会查询给定的`DataSet`,索要下一批次的`batch_size`的图像和标签。\n",
    "```python\n",
    "images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size)\n",
    "\n",
    "feed_dict = {\n",
    "    images_placeholder: images_feed,\n",
    "    labels_placeholder: labels_feed\n",
    "}\n",
    "```\n",
    "\n",
    "5. **检查状态**\n",
    "\n",
    "```python\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "    \n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    # 每隔100步打印训练状态\n",
    "    if step % 100 == 0:\n",
    "        print(xxxxxx)\n",
    "```\n",
    "\n",
    "6. **状态可视化**\n",
    "\n",
    "```python\n",
    "summary_op = tf.merge_all_summaries()\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter(FLAGS.trin_dir, graph_def=sess.graph_def)\n",
    "\n",
    "summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "\n",
    "summary_writer.add_summary(summary_str, step)\n",
    "```\n",
    "\n",
    "7. **保存检查点**\n",
    "\n",
    "```python\n",
    "saver = tf.train.Saver()\n",
    "# 把sess保存到指定文件夹\n",
    "saver.save(sess, FLAGS.train_dir, global_step=step)\n",
    "\n",
    "# 重载\n",
    "saver.restore(sess, FLAGS.train_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4 评估模型\n",
    "\n",
    "每隔一定的训练步骤，会分别对**训练数据集，验证数据集，测试数据集**进行模型评估。\n",
    "1. **构建评估图表**\n",
    "\n",
    "2. **评估图表的输出**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 卷积神经网络\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 运作方式\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 综述\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1 Variables\n",
    "\n",
    "1. **创建variable**\n",
    "\n",
    "```python\n",
    "tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None, constraint=None)\n",
    "\n",
    "```\n",
    "\n",
    "2. **初始化**\n",
    "\n",
    "\n",
    "- `tf.initialize_all_variables()`并行初始化.同时初始化所有变量，用于所有变量没有嵌套的情形。\n",
    "\n",
    "```python\n",
    "weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35), name='wights')\n",
    "biases = tf.Variable(tf.zeros([200]), name='biases')\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "```\n",
    "\n",
    "- `initialized_value()`用于一个variable嵌套一个variable\n",
    "\n",
    "```python\n",
    "weights = tf.Variable(tf.random_normal([...]))\n",
    "\n",
    "w2 = tf.Variable(weights.initialized_value() * 0.2, name='w2')\n",
    "```\n",
    "\n",
    "- `tf.global_variables_initializer()`全局变量初始化\n",
    "\n",
    "\n",
    "3. **保存、重载**\n",
    "\n",
    "\n",
    "- 保存\n",
    "\n",
    "```python\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "init_op = tf.initialize_all_variable()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    save_path = saver.save(sess, './model.ckpt')\n",
    "```\n",
    "\n",
    "- 恢复\n",
    "\n",
    "```python\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model.ckpt')\n",
    "```\n",
    "\n",
    "- 显示保存\n",
    "\n",
    "```python\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "saver = tf.train.Saver({'my_v2': v2})\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2 TensorFlow机制 101(缺失)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3. TensorBoard:学习过程的可视化(ing)\n",
    "\n",
    "```python\n",
    "tf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4. TensorBoard:图的可视化(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 5. 数据读入\n",
    "\n",
    "tensorflow程度读取数据一共有三种方法\n",
    "1. feeding: 在TensorFlow程序运行的每一步，让python代码来供给数据。`tf.placeholder()`\n",
    "2. 从文件读取数据：在TensorFlow图的起始，让一个输入管线从文件中读取数据。\n",
    "\n",
    "```python\n",
    "# 获取文件列表,输出为字符串张量列表\n",
    "tf.train.match_filenames_once(pattern, name=None)\n",
    "\n",
    "# 创建文件队列\n",
    "tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None, cancel_op=None)\n",
    "```\n",
    "\n",
    "3. 预加载数据：在TensorFlow图中定义常量或变量来保存所有数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 6. 线程和队列\n",
    "\n",
    "1. 队列\n",
    "\n",
    "```python\n",
    "tf.FIFOQueue()\n",
    "tf.RandomShuffleQueue()\n",
    "```\n",
    "\n",
    "2. Coordinator\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "3. QueueRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 7. 添加新的Op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 8 自定义数据的Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 9 使用GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10 共享变量\n",
    "\n",
    "```python\n",
    "tf.get_variable(name, shape=None, dtype=None, initializer=None)\n",
    "\n",
    "tf.variable_scope(scope_name)\n",
    "\n",
    "# 常用的初始化方法\n",
    "tf.constant_initializer()\n",
    "tf.random_uniform_initializer()\n",
    "tf.random_normal_initializer()\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
