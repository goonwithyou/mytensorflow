{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 教程\n",
    "---\n",
    "1. MNIST初级教程\n",
    "2. MNIST高级教程\n",
    "3. TensorFlow使用指南\n",
    "4. 卷积神经网络\n",
    "5. 单词的向量表示\n",
    "6. 循环神经网络\n",
    "7. 序列到序列模型\n",
    "8. Mandelbrot集合\n",
    "9. 偏微分方程\n",
    "10. MNIST数据下载\n",
    "11. 视觉物体识别\n",
    "12. Deep Dream\n",
    "\n",
    "# 3 运作方式\n",
    "1. 综述\n",
    "2. 变量\n",
    "3. TensorBoard：可视化学习\n",
    "4. TensorBoard：图标可视化\n",
    "5. 数据读取\n",
    "6. 线程和队列\n",
    "7. 增加一个Op\n",
    "8. 自定义数据读取\n",
    "9. 使用GPUs\n",
    "10. 共享变量\n",
    "\n",
    "# 4 资源\n",
    "1. 其他资源\n",
    "2. BibTex引用\n",
    "3. 应用实例\n",
    "4. 常见问题\n",
    "5. 术语表\n",
    "6. 张量的阶，形状，数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mnist机器学习入门\n",
    "\n",
    "> ### 1 加载Mnist数据集\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('D:/softfiles/workspace/tensorflow/data/', one_hot=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2 softmax回归\n",
    "\n",
    "**softmax模型**可以用来**给不同的对象分配概率**。在进行模型训练的最后一步都常用softmax来分配概率。\n",
    "\n",
    "softmax的定义为：\n",
    "$$softmax(x) = normalize(exp(x))$$\n",
    "展开等式右边的式子：\n",
    "$$softmax(x)_i = \\frac{exp(x_i)}{\\sum_jexp(x_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3 实现回归模型\n",
    "\n",
    "```python\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "```\n",
    "- x不是一个特定的值，而是一个**占位符**placeholder,只需要在运行的时候赋值即可。\n",
    "\n",
    "```python\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros[10])\n",
    "```\n",
    "一个**Variable**代表一个**可修改的tensor**,存在于tensorflow的用于描述交互性操作的图中。**一般模型参数**都可以用**Variable**表示。\n",
    "\n",
    "```python\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4 训练模型\n",
    "\n",
    "1. 定义损失函数\n",
    "\n",
    "定义损失函数评估模型的好坏。常见的成本函数有**交叉熵**。\n",
    "$$H_{y'}=-\\sum_iy'_ilog(y_i)$$\n",
    "- y是预测的概率分布\n",
    "- $y'$是实际的分布\n",
    "\n",
    "**计算交叉熵**\n",
    "```python\n",
    "y_ = tf.placeholder('float', [None, 10])\n",
    "```\n",
    "所以：\n",
    "```python\n",
    "cross_entropy = - tf.reduce_sum(y_ * tf.log(y))\n",
    "```\n",
    "\n",
    "2. 反向传播，更新权重值\n",
    "\n",
    "```python\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "```\n",
    "这里使用梯度下降算法。\n",
    "\n",
    "Tensorflow他会在后台给描述你的计算的那张图里面增加一系列的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。\n",
    "\n",
    "3. 初始化\n",
    "\n",
    "```python\n",
    "init = tf.initiallize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "# 开始训练\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "```\n",
    "随机抓取训练数据中的100个批次的处理数据点，进行训练。\n",
    "\n",
    "4. 模型评估\n",
    "\n",
    "```python\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.raduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "# 测试训练数据，打印正确率\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Mnist优化\n",
    "---\n",
    "> ### 1 权重初始化\n",
    "\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_mormal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "```\n",
    "\n",
    "> ### 2 定义卷积和池化\n",
    "\n",
    "```python\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                        \n",
    "```\n",
    "\n",
    "> ### 3 第一层卷积\n",
    "\n",
    "```python\n",
    "# [5,5,1,32]前两个维度是patch的大小，接着是输入通道的数目，最后是输出的通道数目。\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# [-1, 28, 28, 1]第二三维对应图片的宽和高，最后一维对应图片的颜色通道数。\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# 开始卷积和池化操作\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "```\n",
    "*输出为14 x 14 x 32*\n",
    "\n",
    "> ### 4 第二层卷积\n",
    "\n",
    "```python\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "```\n",
    "*输出为7 x 7 x 64*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 5 密集连接层\n",
    "\n",
    "```python\n",
    "# 加入一个有1024个神经元的全连接层\n",
    "W_fcl = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fcl = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 6 Dropout\n",
    "\n",
    "为了减少过拟合，在输出层之前加入dropout。\n",
    "```python\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fcl_drop = tf.nn.dropout(h_fcl, keep_prob)\n",
    "```\n",
    "\n",
    "> ### 7 输出层\n",
    "\n",
    "```python\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fcl_drop, W_fc2) + b_fc2)\n",
    "```\n",
    "\n",
    "> ### 8 训练和评估模型\n",
    "\n",
    "```python\n",
    "# 交叉熵\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "\n",
    "# 梯度下降\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 判断预测准确性\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "# 准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "# 初始化\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# 开始训练\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # 每100次输出一次准确率\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_:batch[1], keep_prob: 1.0})\n",
    "        print('After %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob: 0.5})\n",
    "\n",
    "# 输出test准确率\n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0\n",
    "}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TensorFlow运作方式\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1 数据准备\n",
    "\n",
    "```python\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "\n",
    "label_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2 构建图表\n",
    "\n",
    "```python\n",
    "def inference():\n",
    "    # 构建图表, 返回预测结果\n",
    "    \n",
    "    # 每一层的操作都创建一个域\n",
    "#     with tf.name_scope('hidden1') as scope;\n",
    "#         weights = tf.Variable(tf.truncated_normal([IMAGE_PIXES],stddev=1.0/math.sqrt(float(IMAGE_PIXES))), name='weight')\n",
    "#         biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "     \n",
    "    # 图表的主要操作\n",
    "    # hidden1 = tf.nn.relu(tf.matmul(images, weights1) + biases1)\n",
    "    # hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    # logits = tf.matmul(hidden2, weights) + biases\n",
    "    pass\n",
    "\n",
    "def loss():\n",
    "    # 往inference图表中添加loss\n",
    "    \n",
    "    # 处理label数据\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    pass\n",
    "\n",
    "def training():\n",
    "    # 添加梯度操作\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
