{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# basic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas\n",
    "---\n",
    "```python\n",
    "# 读取csv数据，设置列名，并将指定列弹出\n",
    "train = pd.read_csv(train_path, names=[], header=0) # return a DataFrame\n",
    "train_x, train_y = train, train.pop([])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---\n",
    "`tf.data`模块主要用于加载数据，预处理和传输到模型中。可以从`numpy.arrays`和csv文件中读取数据。\n",
    "\n",
    "- `Dataset` - Base class\n",
    "- `TextLineDataset` - Reads lines from text file.\n",
    "- `TFRecordDataset` - Reads records from TFRecord files.\n",
    "- `FixedLengthRecordDataset` - Reads fixed size record from binary files.\n",
    "- `Iterator`\n",
    "\n",
    "```python\n",
    "\n",
    "# 把数据集转成Dataset格式\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(features), labels)\n",
    "\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "dataset.make_one_shot_iterator().get_next()\n",
    "```\n",
    "### read data from csv\n",
    "---\n",
    "\n",
    "```python\n",
    "# build dataset\n",
    "ds = tf.data.TextLineDataset(train_path).skip(1)\n",
    "\n",
    "# build a csv line parse\n",
    "COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "       'PetalLength', 'PetalWidth',\n",
    "       'label']\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "    features = dict(zip(COLUMNS, fields))\n",
    "    \n",
    "    label = features.pop('label')\n",
    "    return features, label\n",
    "\n",
    "# parse the lines\n",
    "ds = ds.map(_parse_line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparse\n",
    "---\n",
    "```python\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--name', type=int, default=10, help='')\n",
    "\n",
    "FLAGS, _ = parser.parse_known_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.feature_column\n",
    "---\n",
    "用于将数据转成Estimator可用的格式。\n",
    "```python\n",
    "# 把key转成numeric_column格式\n",
    "tf.feature_column.numeric_column(key)\n",
    "\n",
    "# 列表映射\n",
    "tf.feature_column.categorical_column_with_vocabulary_list(key, vocabulary_list)\n",
    "\n",
    "# 如果不确定列表的取值，可以使用.., 每个值会被映射成数字\n",
    "tf.feature_column.categorical_column_with_hash_bucket(key, hash_bucket_size, dtype=tf.string)\n",
    "\n",
    "# 区间划分\n",
    "tf.feature_column.bucketized_column(source_column, boundaries)\n",
    "\n",
    "# 多列组合成一个keys为包含多个列名的列表，\n",
    "tf.feature_column.crossed_column(keys, hash_bucket_size, hash_key=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator\n",
    "---\n",
    "```python\n",
    "# DNNClassifier\n",
    "classifier = tf.estimator.DNNClassifier(hidden_units, feature_columns, n_classes=2)\n",
    "\n",
    "classifier.train(input_fn, hooks=None, steps=None)\n",
    "classifier.predict(input_fn, predict_keys)\n",
    "classifier.evaluate(input_fn)\n",
    "```\n",
    "---\n",
    "**input_fn**是一个返回Dataset对象的函数，输出应该是有两个元素的元组。\n",
    "- `features` - 一个python的字典类型\n",
    "    - key为特征的名字\n",
    "    - value：包含特征值的列表\n",
    "- `label` - 一个数组，为每个样本的label。\n",
    "\n",
    "```python\n",
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "            'SepalWidth':  np.array([2.8, 2.3]),\n",
    "            'PetalLength': np.array([5.6, 3.3]),\n",
    "            'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels\n",
    "```\n",
    "用法2：\n",
    "```python\n",
    "run_config = tf.estimator.RunConfig(model_dir,...)\n",
    "\n",
    "model_params = tf.contrib.training.HParams()\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn, config=run_config, params=model_params)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec()\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec()\n",
    "```\n",
    "\n",
    "## Define the model\n",
    "---\n",
    "### Define the input layer\n",
    "```python\n",
    "# write an input function\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "# Create feature columns\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "# Write a model fnction\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    # Define the input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    # Define hidden layer\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    # Define Output layer\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "    return logits\n",
    "\n",
    "# Define classifier\n",
    "classifier = tf.estimator.Estimator(model_fn=my_model_fn, \n",
    "                                    params={\n",
    "                                        'feature_column': my_feature_columns,\n",
    "                                        'hidden_units': [10, 10],\n",
    "                                        'n_classes': 3\n",
    "                                    })\n",
    "\n",
    "# Define train\n",
    "classifier.train(input_fn=lambda: train_input_fn(FILE_TRAIN, True, 500))# MOdeKeys.TRAIN\n",
    "\n",
    "# Define predict operation\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions={\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "# loss function\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "# Evaluate operation\n",
    "accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes, name='acc_op')\n",
    "\n",
    "metrics = {'accuracy': accuracy}\n",
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "# Train operation\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.logging\n",
    "---\n",
    "```python\n",
    "# 打开日志功能\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoints\n",
    "---\n",
    "用于estimator保存模型,配置保存间隔，和最大保存文件数。\n",
    "```python\n",
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "    save_checkpoint_secs=20*60,\n",
    "    keep_checkpoint_max=10\n",
    ")\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3,\n",
    "    model_dir='xxx/xxx'\n",
    "    config=my_checkpointing_config\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.layers\n",
    "---\n",
    "该模块主要用于创建神经网络，提供了创建全连接层，卷积层，激活函数，dropout regularization。案例为**mnist**\n",
    "\n",
    "> CNNS(conv(relu)-pool-conv(relu)-pool-...-conv(relu)-dense-dense-output)\n",
    "\n",
    "- **Convolutional layers**,in the last will apply RELU activation function to the output.\n",
    "- **Pooling**, reduce the dimensionality.\n",
    "- **Dense**, which perform classification on the features.\n",
    "\n",
    "> build a model to classify the images in the mnist dataset.\n",
    "\n",
    "1. conv1, `weights = [-1, 5, 5, 32]`, with ReLU\n",
    "2. pool1, `kernel_size=[1, 2, 2, 1]`, `stride=[1, 2, 2, 1]`\n",
    "3. conv2, `weights=[-1, 5, 5, 64]`, with ReLU\n",
    "4. pool2, `kernel_size=[1, 2, 2, 1]`, `stride=[1, 2, 2, 1]`\n",
    "5. dense1, `[-1, 1024]`, `dropout(0.4)`,\n",
    "6. dense2, `[-1, 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.DNNClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tile?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
