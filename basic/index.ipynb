{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# basic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas\n",
    "---\n",
    "```python\n",
    "# 读取csv数据，设置列名，并将指定列弹出\n",
    "train = pd.read_csv(train_path, names=[], header=0) # return a DataFrame\n",
    "train_x, train_y = train, train.pop([])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "---\n",
    "- tf.Variable\n",
    "- tf.constant\n",
    "- tf.placeholder\n",
    "- tf.SparseTensor\n",
    "\n",
    "> Rank\n",
    "\n",
    "- rank0: 标量\n",
    "- rank1: list\n",
    "- rank2: ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable\n",
    "---\n",
    "tf.Variable保存了一个持久化的tensor，ops可以读取和修改这个tensor的值。\n",
    "\n",
    "> ## 1 Creating a Variable\n",
    "\n",
    "最好的方法是通过`tf.get_variable`来创建和使用variable，这个函数需要指定一个名字。\n",
    "```python\n",
    "my_variable = tf.get_variable('my_variable', [1, 2, 3])\n",
    "\n",
    "my_int_variable = tf.get_variable('my_int_variable', [1, 2, 3],\n",
    "                       dtype=tf.int32,\n",
    "                       initializer=tf.zeros_initializer)\n",
    "```\n",
    "\n",
    "> ## 2 Variable collections\n",
    "\n",
    "`tf.GraphKeys`\n",
    "\n",
    "不训练的变量可以加入到`tf.GraphKeys.LOCAL_VARIABLES`\n",
    "```python\n",
    "my_local = tf.get_variable('my_local', shape=[], collections=[tf.GraphKeys.LOCAL_VARIABLES])\n",
    "# or\n",
    "my_non_trainable = tf.get_variable('my_non_trainable', shape=(), trainable=False)\n",
    "\n",
    "tf.add_to_collection('xx', my_local)\n",
    "\n",
    "tf.get_collection('xx')\n",
    "```\n",
    "\n",
    "> ## 3 Device placement\n",
    "\n",
    "```python\n",
    "with tf.device('/device:GPU:1'):\n",
    "    v = tf.get_variable('v', [1])\n",
    "\n",
    "cluster_spec = {\n",
    "    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n",
    "    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\n",
    "with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\n",
    "  v = tf.get_variable(\"v\", shape=[20, 20])  # this variable is placed\n",
    "                                            # in the parameter server\n",
    "                                            # by the replica_device_setter\n",
    "```\n",
    "\n",
    "> ## 4 Initializing variable\n",
    "\n",
    "```python\n",
    "tf.global_variables_initializer()\n",
    "```\n",
    "> ## 5 Using variables\n",
    "\n",
    "> ## 6 Sharing variables\n",
    "\n",
    "```python\n",
    "with tf.variable_scope(\"model\"):\n",
    "  output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "  output2 = my_image_filter(input2)\n",
    "  \n",
    "with tf.variable_scope(\"model\") as scope:\n",
    "  output1 = my_image_filter(input1)\n",
    "  scope.reuse_variables()\n",
    "  output2 = my_image_filter(input2)\n",
    "  \n",
    "with tf.variable_scope(\"model\") as scope:\n",
    "  output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(scope, reuse=True):\n",
    "  output2 = my_image_filter(input2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "---\n",
    "> ## 1 Dataflow\n",
    "\n",
    "tensorflow 使用dataflow来进行模型计算。dataflow的优势主要有\n",
    "- 并行处理\n",
    "- 分布式\n",
    "- 预编译，数据处理速度更快\n",
    "- 可保存\n",
    "\n",
    "> ## 2 tf.Graph\n",
    "\n",
    "1. Graph structure\n",
    "    \n",
    "    tensorflow所有的操作步骤都记录在graph上，graph主要包含两个部分\n",
    "    - tf.Opearteion:也称ops，graph上的所有节点都是一个ops，他记录了操作行为，如加减乘除等。\n",
    "    - tf.Tensor：graph上的边都是一个tensor，表示了节点计算所需要的值。\n",
    "\n",
    "    **tensor**并不记录值，只是一个计算图的组成部分，只记录了值的类型和shape。以及值的引用。\n",
    "2. Graph collections\n",
    "    - tensorflow定义了一些集合用来存储模型的临时数据，`tf.add_to_collection`可以将值加入到指定的集合(已定义的集合名有`tf.GraphKeys`),`tf.get_collection`获取集合中的值。\n",
    "\n",
    "> ## 3 building a graph\n",
    "\n",
    "- `tf.constant(32)`创建一个用于生成32的ops，并加入到默认graph中，返回一个tensor\n",
    "- `tf.matmul(x, y)`创建一个两个tensor相乘的ops，加入到默认graph中，return相乘结果。\n",
    "- `v = tf.Variable(0)`创建一个可修改值的tensor到graph中。\n",
    "- `tf.train.Optimizer.minimizer`会把tensors和operations加入到graph用来计算梯度，并返回一个ops，可以用于更新variable\n",
    "\n",
    "通常我们也会创建不同的graph用于train和eval。\n",
    "\n",
    "> ## 4 Nameing operations\n",
    "\n",
    "Graph为ops定义了**namespace**。主要有两种方法为ops定义name：\n",
    "- `tf.constant(2, name='test')`创建一个名为test的用于生成2的ops，\"test:0\"\n",
    "- `tf.name_scope('xxx')`用于为该context下所有ops加入一个命名前缀\n",
    "\n",
    "**如果有重名的ops定义，为保证命名唯一，会在名字后加入'_1','_2'...**\n",
    "\n",
    "**命名会在使用tensorboard的时候，简化阅读**\n",
    "\n",
    "> ## 5 placing operations on different devices\n",
    "\n",
    "`tf.device`用于指定ops在哪个device上运行。\n",
    "\n",
    "如果在单个cpu和gpu上使用\n",
    "```python\n",
    "weights = tf.random_normal(...)\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    img = tf.decode_jpg('xx')\n",
    "with tf.device('/device:GPU:0'):\n",
    "    result = tf.matmull(weights, img)\n",
    "```\n",
    "如果使用了分布式，要定义job name and task ID，,Variable指定给job/ps, ops指定给job/worker\n",
    "```python\n",
    "with tf.device('/job:ps/task:0'):\n",
    "    weights_1 = tf.Variable(..)\n",
    "    biases_1 = tf.Variable(...)\n",
    "\n",
    "with tf.device('/job:ps/task:1'):\n",
    "    weights_1 = tf.Variable(...)\n",
    "    biases_1 = tf.Variable(...)\n",
    "    \n",
    "with tf.device('/jpb:worker'):\n",
    "    layer_1 = tf.matmul(train_batch, weights_1) + biases_1\n",
    "    layer_2 = tf.matmul(train_batch, weights_2) + biases_2\n",
    "```\n",
    "也可以使用简单的方法`tf.train.replica_device_setter()`\n",
    "```python\n",
    "with tf.device(tf.train.replica_device_setter(ps_tasks=3)):\n",
    "  # tf.Variable objects are, by default, placed on tasks in \"/job:ps\" in a\n",
    "  # round-robin fashion.\n",
    "  w_0 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\n",
    "  b_0 = tf.Variable(...)  # placed on \"/job:ps/task:1\"\n",
    "  w_1 = tf.Variable(...)  # placed on \"/job:ps/task:2\"\n",
    "  b_1 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\n",
    "\n",
    "  input_data = tf.placeholder(tf.float32)     # placed on \"/job:worker\"\n",
    "  layer_0 = tf.matmul(input_data, w_0) + b_0  # placed on \"/job:worker\"\n",
    "  layer_1 = tf.matmul(layer_0, w_1) + b_1     # placed on \"/job:worker\"\n",
    "```\n",
    "\n",
    "> ## 6 tensor-like objects\n",
    "\n",
    "tensor是tensorflow的基本执行单元，为方便操作，一些tensor-like也可以作为tensor使用。\n",
    "- tf.Tensor\n",
    "- tf.Variable\n",
    "- numpy.ndarray\n",
    "- list\n",
    "- scalar python type:bool, float, int, str\n",
    "\n",
    "每次使用tensor-like会默认创建一个tensor object。如果tensor-like太大，需要使用`tf.convert_to_tensor` 来避免内存溢出。\n",
    "\n",
    "> ## 7 Execution a graph in a tf.Session\n",
    "\n",
    "1. Creating a tf.Session\n",
    "\n",
    "```python\n",
    "# create a default in-process session\n",
    "with tf.Session() as sess:\n",
    "    ...\n",
    "\n",
    "# create a remote session\n",
    "with tf.Session('grpc://example.org:2222'):\n",
    "    ...\n",
    "```\n",
    "\n",
    "在一些高级API中如`tf.train.MonitoredTrainingSession`or`tf.estimator.Estimator`会自动创建和管理一个session。这些API接收一些参数操作sess\n",
    "- `target`如果为空则表示使用本地device，可以指定一个远程server的'grpc://'URL。\n",
    "- `graph`默认会使用当前graph，如果使用多个graph，也可特别指定。\n",
    "- `config`接收一个`tf.ConfigProto`来控制session，ConfigProto主要 有以下参数：\n",
    "    - `allow_soft_placement`:如果设为True，则会有Gpu时使用gpu，没有则用cpu\n",
    "    - `cluster_def`:在分布式处理时，能显示其他机器的运行情况\n",
    "    - `graph_option.optimizer_options`: 指定optimization\n",
    "    - `gpu_options.allow_growth`:设为True，gpu的内存会可变。\n",
    "    \n",
    "2. Useing tf.Session.run to execute operations\n",
    "\n",
    "`sess.run`主要用于running a ops or eval a tensor.\n",
    "\n",
    "> ## 8 Visualizing your graph\n",
    "\n",
    "```python\n",
    "# Build your graph.\n",
    "x = tf.constant([[37.0, -23.0], [1.0, 4.0]])\n",
    "w = tf.Variable(tf.random_uniform([2, 2]))\n",
    "y = tf.matmul(x, w)\n",
    "# ...\n",
    "loss = ...\n",
    "train_op = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # `sess.graph` provides access to the graph used in a `tf.Session`.\n",
    "  writer = tf.summary.FileWriter(\"/tmp/log/...\", sess.graph)\n",
    "\n",
    "  # Perform your computation...\n",
    "  for i in range(1000):\n",
    "    sess.run(train_op)\n",
    "    # ...\n",
    "\n",
    "  writer.close()\n",
    "```\n",
    "\n",
    "> ## 9 Programming with multiple graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TesnsorBoard\n",
    "---\n",
    "图形化显示graph,\n",
    "```python\n",
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "retulst = a + b\n",
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "# 终端运行 tensorboard --logdir=path\n",
    "# path为当前路径\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session\n",
    "---\n",
    "tensorflow中的graph就类似于python中的.py文件,tf.Session就类似于python，用于执行graph。\n",
    "```python\n",
    "sess = tf.Session()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding\n",
    "---\n",
    "为了减少在模型训练中tensor的多次创建。tf.placeholder用于创建一个占位，类似于函数的参数，只需要的在使用的时候传进去就可以。\n",
    "```python\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x:3, y:4.5}))\n",
    "    print(sess.run(z, feed_dict={x:[3, 4], y:[4, 5]}))\n",
    "```\n",
    "**创建placeholder后，一定要在执行的过程中使用feed_dict传值。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "retulst = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---\n",
    "`tf.data`模块主要用于加载数据，预处理和传输到模型中。可以从`numpy.arrays`和csv文件中读取数据。想要使用dataset中的tensor需要先创建一个**iterator**用`dataset.make_one_shot_iterator()`方法，然后使用`get_next()`获取数据。如果get_next()获取不到数据，会抛出`OutOfRangeError`错误。\n",
    "\n",
    "- `Dataset` - Base class\n",
    "- `TextLineDataset` - Reads lines from text file.\n",
    "- `TFRecordDataset` - Reads records from TFRecord files.\n",
    "- `FixedLengthRecordDataset` - Reads fixed size record from binary files.\n",
    "- `Iterator`\n",
    "\n",
    "> ## 1 Basic mechanics\n",
    "\n",
    "1. define a source, build a **Dataset**.use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()` or `tf.data.TFRecordDataset()`\n",
    "2. transform into a new dataset by chaining method. `Dataset.map()` and `Dataset.batch()`\n",
    "3. make an **iterator**.`Dataset.make_one_shot_iterator()`需要使用`Iterator.initializer()`对iterator进行初始化，`Iterator.get_next()`来取值。\n",
    "\n",
    "> ## 2 Dataset structure\n",
    "\n",
    "每个dataset有`Dataset.output_types` and `Dataset.output_shapes`两个属性。\n",
    "```python\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\"\n",
    "```\n",
    "使用dict\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n",
    "```\n",
    "使用map\n",
    "```python\n",
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "\n",
    "# Note: Argument destructuring is not available in Python 3.\n",
    "dataset3 = dataset3.filter(lambda x, (y, z): ...)\n",
    "```\n",
    "\n",
    "> ## 3 Creating an iterator\n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value\n",
    "    \n",
    "######initializable,可用于带参迭代#########\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "# 初始化迭代器，每次返回10个elements\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "\n",
    "# 初始化迭代器，每次返回100个elements\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "```\n",
    "\n",
    "> ### 3.1 reinitializable\n",
    "\n",
    "> ### 3.2 Consuming values from an iterator\n",
    "\n",
    "```python\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "sess.run(iterator.initializer)\n",
    "```\n",
    "\n",
    "```python\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                            training_dataset.output_shapes)\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "for _ in range(20):\n",
    "    sess.run(training_init_op)\n",
    "    for _ in range(100):\n",
    "        sess.run(next_element)\n",
    "        \n",
    "    sess.run(validation_init_op)\n",
    "    for _ in range(50):\n",
    "        sess.run(next_element)\n",
    "```\n",
    "> ### 3.3 feedable iterator\n",
    "\n",
    "\n",
    "> ## 4 Reading input data\n",
    "\n",
    "> ### 4.1 Consuming Numpy arrays\n",
    "\n",
    "```python\n",
    "with np.load('xxx.npy') as data:\n",
    "    feature = data['feature']\n",
    "    labels = data['labels']\n",
    "    \n",
    "assert feature.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(feature.dtype, feature.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataseeet.from_tensor_slices((feature_placeholder, labels_placeholder))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})\n",
    "```\n",
    "\n",
    "> ### 4.2 Consuming TFRecord data\n",
    "\n",
    "```python\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map().repeat().batch()\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "validation_filenames = [\"/var/data/validation1.tfrecord\", ...]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})\n",
    "```\n",
    "\n",
    "> ### 4.3 Consuming text data\n",
    "\n",
    "```python\n",
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "# Use `Dataset.flat_map()` to transform each file as a separate nested dataset,\n",
    "# and then concatenate their contents sequentially into a single \"flat\" dataset.\n",
    "# * Skip the first line (header row).\n",
    "# * Filter out lines beginning with \"#\" (comments).\n",
    "dataset = dataset.flat_map(\n",
    "    lambda filename: (\n",
    "        tf.data.TextLineDataset(filename)\n",
    "        .skip(1)\n",
    "        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))\n",
    "```\n",
    "\n",
    "> ### 4.4 read data from csv\n",
    "\n",
    "```python\n",
    "# build dataset\n",
    "ds = tf.data.TextLineDataset(train_path).skip(1)\n",
    "\n",
    "# build a csv line parse\n",
    "COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "       'PetalLength', 'PetalWidth',\n",
    "       'label']\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "    features = dict(zip(COLUMNS, fields))\n",
    "    \n",
    "    label = features.pop('label')\n",
    "    return features, label\n",
    "\n",
    "# parse the lines\n",
    "ds = ds.map(_parse_line)\n",
    "```\n",
    "> ## 5 Preprocessing data with `Dataset.map()`\n",
    "\n",
    "> ### 5.1 Parsing `tf.Example` protocol buffer message\n",
    "\n",
    "```python\n",
    "def _parse_function(example_proto):\n",
    "    features = {'image': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "            'label': tf.FixedLenFeature((), tf.int32, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features['image'], parse_feature['label']\n",
    "\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)\n",
    "```\n",
    "> ### 5.2 Decoding image data and resizing it\n",
    "\n",
    "```python\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_image(image_string)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resize, label\n",
    "\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n",
    "\n",
    "labels = tf.constant([0, 37, ...])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)\n",
    "```\n",
    "> ### 5.3 Applying arbitrary Python logic with `tf.py_func()`\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# Use a custom OpenCV function to read the image, instead of the standard\n",
    "# TensorFlow `tf.read_file()` operation.\n",
    "def _read_py_function(filename, label):\n",
    "  image_decoded = cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)\n",
    "  return image_decoded, label\n",
    "\n",
    "# Use standard TensorFlow operations to resize the image to a fixed shape.\n",
    "def _resize_function(image_decoded, label):\n",
    "  image_decoded.set_shape([None, None, None])\n",
    "  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "  return image_resized, label\n",
    "\n",
    "filenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]\n",
    "labels = [0, 37, 29, 1, ...]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(\n",
    "    lambda filename, label: tuple(tf.py_func(\n",
    "        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\n",
    "dataset = dataset.map(_resize_function)\n",
    "```\n",
    "> ## 6 Batching dataset elements\n",
    "\n",
    "> ### 6.1 Simple batching\n",
    "\n",
    "`Dataset.batch()`\n",
    "> ### 6.2 Batching tensors with padding\n",
    "\n",
    "```python\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None])\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n",
    "                               #      [5, 5, 5, 5, 5, 0, 0],\n",
    "                               #      [6, 6, 6, 6, 6, 6, 0],\n",
    "                               #      [7, 7, 7, 7, 7, 7, 7]]\n",
    "```\n",
    "> ## 7 Training workflows\n",
    "\n",
    "> ### 7.1 Processing multiple epochs\n",
    "\n",
    "可以使用dataset.repeat(),或者初始化多次。\n",
    "```python\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.repeat(10)\n",
    "dataset = dataset.batch(32)\n",
    "```\n",
    "> ### 7.2 Randomly shuffling input data\n",
    "\n",
    "`dataset = dataset.shuffle(buffer_size=10000)`\n",
    "\n",
    "> ### 7.3 Using high-level APIs\n",
    "\n",
    "```python\n",
    "def dataset_input_fn():\n",
    "  filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "  dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "  # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "  # protocol buffer, and perform any additional per-record preprocessing.\n",
    "  def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # Perform additional preprocessing on the parsed data.\n",
    "    image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "    image = tf.reshape(image, [299, 299, 1])\n",
    "    label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "    return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "  # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "  # tensor for each example.\n",
    "  dataset = dataset.map(parser)\n",
    "  dataset = dataset.shuffle(buffer_size=10000)\n",
    "  dataset = dataset.batch(32)\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "  # `features` is a dictionary in which each value is a batch of values for\n",
    "  # that feature; `labels` is a batch of labels.\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparse\n",
    "---\n",
    "```python\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--name', type=int, default=10, help='')\n",
    "\n",
    "FLAGS, _ = parser.parse_known_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.feature_column\n",
    "---\n",
    "feature_column用于tf.feature_column.input_layer函数。\n",
    "```python\n",
    "# 把key转成numeric_column格式\n",
    "tf.feature_column.numeric_column(key)\n",
    "\n",
    "# 列表映射\n",
    "tf.feature_column.categorical_column_with_vocabulary_list(key, vocabulary_list)\n",
    "\n",
    "# 如果不确定列表的取值，可以使用.., 每个值会被映射成数字\n",
    "tf.feature_column.categorical_column_with_hash_bucket(key, hash_bucket_size, dtype=tf.string)\n",
    "\n",
    "# 区间划分\n",
    "tf.feature_column.bucketized_column(source_column, boundaries)\n",
    "\n",
    "# 多列组合成一个keys为包含多个列名的列表，\n",
    "tf.feature_column.crossed_column(keys, hash_bucket_size, hash_key=None)\n",
    "```\n",
    "> eg:\n",
    "\n",
    "```python\n",
    "features = {\n",
    "    'sales': [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'grad', 'grad']\n",
    "}\n",
    "depart = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'department',['sports', 'grad'])\n",
    "# indicator_column用于包装任何‘categorical_column_*’\n",
    "depart = tf.feature_column.indicator_column(depart)\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    depart\n",
    "]\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # feature column需要tables_initializer初始化\n",
    "    tf.tables_initializer().run()\n",
    "    print(sess.run(inputs))\n",
    "```\n",
    "输出为\n",
    "```\n",
    "[[ 1.  0.  5.]\n",
    " [ 1.  0. 10.]\n",
    " [ 0.  1.  8.]\n",
    " [ 0.  1.  9.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easy linear train model\n",
    "---\n",
    "```python\n",
    "# 创建输入值和target值\n",
    "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "y_ = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\n",
    "\n",
    "# create linear model\n",
    "y_pred = tf.layers.dense(x, 1)\n",
    "\n",
    "# MSE loss\n",
    "loss = tf.losses.mean_squared_error(labels=y_, predictions=y_pred)\n",
    "\n",
    "# optimizer\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        _, loss_value = sess.run((train_op, loss))\n",
    "        print(loss_value)\n",
    "    \n",
    "    print(sess.run(y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "---\n",
    "Estimator主要有以下action：\n",
    "- training\n",
    "- evaluation\n",
    "- prediction\n",
    "- export for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator\n",
    "---\n",
    "```python\n",
    "# DNNClassifier\n",
    "classifier = tf.estimator.DNNClassifier(hidden_units, feature_columns, n_classes=2)\n",
    "\n",
    "classifier.train(input_fn, hooks=None, steps=None)\n",
    "classifier.predict(input_fn, predict_keys)\n",
    "classifier.evaluate(input_fn)\n",
    "```\n",
    "---\n",
    "**input_fn**是一个返回Dataset对象的函数，输出应该是有两个元素的元组。\n",
    "- `features` - 一个python的字典类型\n",
    "    - key为特征的名字\n",
    "    - value：包含特征值的列表\n",
    "- `label` - 一个数组，为每个样本的label。\n",
    "\n",
    "```python\n",
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "            'SepalWidth':  np.array([2.8, 2.3]),\n",
    "            'PetalLength': np.array([5.6, 3.3]),\n",
    "            'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels\n",
    "```\n",
    "用法2：\n",
    "```python\n",
    "run_config = tf.estimator.RunConfig(model_dir,...)\n",
    "\n",
    "model_params = tf.contrib.training.HParams()\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn, config=run_config, params=model_params)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec()\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec()\n",
    "```\n",
    "\n",
    "## Define the model\n",
    "---\n",
    "### Define the input layer\n",
    "```python\n",
    "# write an input function\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "# Create feature columns\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "# Write a model fnction\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    # Define the input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    # Define hidden layer\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    # Define Output layer\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "    return logits\n",
    "\n",
    "# Define classifier\n",
    "classifier = tf.estimator.Estimator(model_fn=my_model_fn, \n",
    "                                    params={\n",
    "                                        'feature_column': my_feature_columns,\n",
    "                                        'hidden_units': [10, 10],\n",
    "                                        'n_classes': 3\n",
    "                                    })\n",
    "\n",
    "# Define train\n",
    "classifier.train(input_fn=lambda: train_input_fn(FILE_TRAIN, True, 500))# MOdeKeys.TRAIN\n",
    "\n",
    "# Define predict operation\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions={\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "# loss function\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "# Evaluate operation\n",
    "accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes, name='acc_op')\n",
    "\n",
    "metrics = {'accuracy': accuracy}\n",
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "# Train operation\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.logging\n",
    "---\n",
    "```python\n",
    "# 打开日志功能\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoints\n",
    "---\n",
    "用于estimator保存模型,配置保存间隔，和最大保存文件数。\n",
    "```python\n",
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "    save_checkpoint_secs=20*60,\n",
    "    keep_checkpoint_max=10\n",
    ")\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3,\n",
    "    model_dir='xxx/xxx'\n",
    "    config=my_checkpointing_config\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.layers\n",
    "---\n",
    "一个可训练的模型必须实现对graph中的参数值进行修改(以实现每次训练模型后相同的输入不同的输出)，对graph中的trainable参数进行更新。\n",
    "\n",
    "该模块主要用于创建神经网络，提供了创建全连接层，卷积层，激活函数，dropout regularization。案例为**mnist**\n",
    "\n",
    "> CNNS(conv(relu)-pool-conv(relu)-pool-...-conv(relu)-dense-dense-output)\n",
    "\n",
    "- **Convolutional layers**,in the last will apply RELU activation function to the output.\n",
    "- **Pooling**, reduce the dimensionality.\n",
    "- **Dense**, which perform classification on the features.\n",
    "\n",
    "> build a model to classify the images in the mnist dataset.\n",
    "\n",
    "1. conv1, `weights = [-1, 5, 5, 32]`, with ReLU\n",
    "2. pool1, `kernel_size=[1, 2, 2, 1]`, `stride=[1, 2, 2, 1]`\n",
    "3. conv2, `weights=[-1, 5, 5, 64]`, with ReLU\n",
    "4. pool2, `kernel_size=[1, 2, 2, 1]`, `stride=[1, 2, 2, 1]`\n",
    "5. dense1, `[-1, 1024]`, `dropout(0.4)`,\n",
    "6. dense2, `[-1, 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.DNNClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
